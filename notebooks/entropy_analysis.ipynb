{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208290d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from adaptation.entropy_calculator import EntropyCalculator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b0272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "entropy_calc = EntropyCalculator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8171a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"easy\": \"The capital of France is\",\n",
    "    \"medium\": \"The theory of evolution explains\",\n",
    "    \"hard\": \"In a universe governed by probabilistic causality\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c25b0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_entropy_trajectory(prompt, steps=20):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    entropies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, use_cache=True, return_dict=True)\n",
    "        kv_cache = outputs.past_key_values\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        for _ in range(steps):\n",
    "            entropy = entropy_calc.compute(logits).item()\n",
    "            entropies.append(entropy)\n",
    "\n",
    "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            outputs = model(\n",
    "                token,\n",
    "                past_key_values=kv_cache,\n",
    "                use_cache=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            kv_cache = outputs.past_key_values\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    return entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b22e2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "for name, prompt in prompts.items():\n",
    "    ent = compute_entropy_trajectory(prompt)\n",
    "    plt.plot(ent, label=name)\n",
    "\n",
    "plt.xlabel(\"Decoding Step\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Entropy over Decoding Steps\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
