{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50f49f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from core.speculative_engine import SpeculativeEngine\n",
    "from core.draft_model import DraftModel\n",
    "from core.target_model import TargetModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04732dad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_DRAFT  = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MODEL_TARGET = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TARGET)\n",
    "draft  = DraftModel(tokenizer, MODEL_DRAFT,  device=device, dtype=torch.float16)\n",
    "target = TargetModel(tokenizer, MODEL_TARGET, device=device, dtype=torch.float16)\n",
    "\n",
    "prompt = \"The theory of evolution explains\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "MAX_TOKENS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc41ea9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "target.kv_cache = None\n",
    "target.position = 0\n",
    "target.init_kv_cache(input_ids)\n",
    "\n",
    "vanilla_tokens = []\n",
    "last_token = input_ids[:, -1:]\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(MAX_TOKENS):\n",
    "        logits = target.forward_next(last_token)\n",
    "        next_token = target.select_tokens(logits)\n",
    "        vanilla_tokens.append(next_token.item())\n",
    "        last_token = next_token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8aa9e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "vanilla_time = time.time() - start\n",
    "vanilla_latency = vanilla_time / len(vanilla_tokens) * 1000\n",
    "\n",
    "print(\"=== FAIR VANILLA BASELINE ===\")\n",
    "print(f\"Total time (s): {vanilla_time:.4f}\")\n",
    "print(f\"Tokens generated: {len(vanilla_tokens)}\")\n",
    "print(f\"Latency per token (ms): {vanilla_latency:.2f}\")\n",
    "print(f\"Throughput (tok/s): {len(vanilla_tokens)/vanilla_time:.2f}\")\n",
    "\n",
    "vanilla_text = tokenizer.decode(vanilla_tokens, skip_special_tokens=True)\n",
    "print(f\"Output: {vanilla_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a85bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "engine = SpeculativeEngine(\n",
    "    draft_model=draft,\n",
    "    target_model=target,\n",
    "    max_k=4,\n",
    "    entropy_bins=[1.5, 3.0, 5.0],\n",
    "    k_values=[4, 3, 2, 1],\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "spec_output = engine.decode(input_ids, max_tokens=MAX_TOKENS)\n",
    "spec_latency = engine.performance_tracker.summary()['latency_per_token_ms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ba0de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== SPECULATIVE ENGINE ===\")\n",
    "print(engine.performance_tracker.summary())\n",
    "print(engine.quality_evaluator.summary())\n",
    "print(f\"Output: {tokenizer.decode(spec_output[0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8378211",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Vanilla\\n(fair)\", \"Speculative\"], [vanilla_latency, spec_latency])\n",
    "plt.ylabel(\"Latency per Token (ms)\")\n",
    "plt.title(\"Latency Comparison (Fair)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSpeedup: {vanilla_latency/spec_latency:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
